import json
import math
import multiprocessing
import os
import time
import pefile
from opcodes.opcodeGramSelection import getMD


class CustomError(Exception):
    """自定义异常类型的示例"""

    def __init__(self, message):
        self.message = message
        super().__init__(self.message)


def dictSort(dictionary):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def fileParse(features: list, validPath: str):
    s = time.time()
    try:
        pe = pefile.PE(validPath)  # 加载 PE 文件
        md = getMD(pe)
        opcodeList = []
        for section in pe.sections:
            if not section.IMAGE_SCN_MEM_EXECUTE:  # 如果不可执行那么就检查下一个段
                continue
            section_data = section.get_data()
            section_addr = section.VirtualAddress
            # 反汇编节的内容
            for ins in md.disasm(section_data, section_addr):
                opcodeList.append(ins.mnemonic)
        opcodeSequence = ' '.join(opcodeList)

    except CustomError('Can not disassemble ' + validPath + ', check the configs of PEFILE and Capstone'):
        return None  # 无法反汇编的直接返回None, 且不能计入总文档数n

    currentIDFTable = {}
    for feature in features:
        # 此特征在这个文档中出现了，不论出现几次都记为1, 若此特征在这个文档中没有出现则记为0
        if feature in opcodeSequence:
            currentIDFTable[feature] = 1
        else:
            currentIDFTable[feature] = 0

    e = time.time()
    print('files name:', validPath, ", opcode length:", len(opcodeList),
          ', time cost:', round(e - s, 6), ', parsed by process:', os.getpid())

    return currentIDFTable
    # 此时完成1整个二进制文件的n-grams读取，即统计了此样本都存在哪些n-grams，但没有计数出现频数


def main():
    # 读入json文件，确定要取的样本特征类型
    with open(r"feature-IG-pairs.json", "r") as f:
        content = json.load(f)
    features = list(content.keys())

    with open("../config.json") as f:
        configs = json.load(f)
    print('Creating IDF Table based on train and validation set:')

    # 指定在哪些数据集上为features计算IDF
    inWhichDatasets = [configs['malicious_train_samples_path'],
                       configs['benign_train_samples_path'],
                       configs['malicious_validation_samples_path'],
                       configs['benign_validation_samples_path']]

    validPaths = {}
    for root in inWhichDatasets:
        paths = os.listdir(root)
        for path in paths:
            fullPath = os.path.join(root, path)
            validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)

    fileParsePool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    resultAddress = []
    for validPath in validPaths.keys():
        resultAddress.append(fileParsePool.apply_async(fileParse, args=(features, validPath)))
    fileParsePool.close()
    fileParsePool.join()

    IDFTable = {key: 0 for key in features}
    dicts = []
    for address in resultAddress:
        if address.get() is not None:  # 无法反汇编的文件返回的是None，不可计入总文档数
            dicts.append(address.get())

    n = len(dicts)  # 无法反汇编的文件返回的是None，不可计入总文档数，这里是真正的可用总文档数

    for feature in features:
        for item in dicts:
            IDFTable[feature] += item[feature]

    finalIDFDict = {key: math.log2(n / (value + 1)) for key, value in IDFTable.items()}

    finalIDFDict = dictSort(finalIDFDict)

    jsonPath = 'feature-IDF-pairs.json'
    if os.path.exists(jsonPath):
        os.remove(jsonPath)

    with open(jsonPath, 'w') as json_file:
        json.dump(finalIDFDict, json_file)  # 应该注意的是，运行多次生成的文件可以是不同，因为IG重复值很多，然后降序排序后的结果不唯一

    print("Successfully output the feature-IDF-pairs.json as the IDF for each feature")
    print()
