import collections
import json
import math
import multiprocessing
import os
import time
from capstone import *
import pefile


class CustomError(Exception):
    """自定义异常类型的示例"""

    def __init__(self, message):
        self.message = message
        super().__init__(self.message)


def entropy(pi):
    return -pi * math.log2(pi)


def dictSort(dictionary):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def getMD(pe):
    # The optional header magic number determines whether an image is a PE32 or PE32+ executable.
    try:
        mode = CS_MODE_32 if pe.OPTIONAL_HEADER.Magic == pefile.OPTIONAL_HEADER_MAGIC_PE else CS_MODE_64
    except CustomError('Can not get hardware mode'):
        return None

    if pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_UNKNOWN']:
        print('Can not get hardware arch')
        return None

    # x86和x86_64
    elif pe.FILE_HEADER.Machine in [pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_I386'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_AMD64']]:
        arch = CS_ARCH_X86  # 应该是包含了X86和X86_64

    # arm
    elif pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_ARM']:
        arch = CS_ARCH_ARM

    # arm64
    elif pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_ARM64']:
        arch = CS_ARCH_ARM64

    # MIPS
    elif pe.FILE_HEADER.Machine in [pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_R3000'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_R4000'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_R10000'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_WCEMIPSV2'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_MIPS16'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_MIPSFPU'],
                                    pefile.MACHINE_TYPE['IMAGE_FILE_MACHINE_MIPSFPU16']]:
        arch = CS_ARCH_MIPS

    # PowerPC
    elif 'POWERPC' in pefile.MACHINE_TYPE[pe.FILE_HEADER.Machine]:
        arch = CS_ARCH_PPC

    else:  # 无法得到目前所支持的指令架构
        print('Can not get supported hardware arch')
        return None

    return Cs(arch, mode)


def dictUpdate(dictionary: dict, grams: list):
    s = time.time()
    element_counts = collections.Counter(grams)
    for key, value in element_counts.items():
        dictionary[key] = value
    e = time.time()
    print('dict size:', len(dictionary), ', update time:', round(e - s, 6))


def fileParse(n: int, validPath: str):
    s = time.time()
    try:  # 存在无法反汇编的文件，原因可能是PEFILE和Capstone的配置存在问题
        pe = pefile.PE(validPath)  # 加载 PE 文件
        md = getMD(pe)  # 可能为None
        opcodeList = []
        for section in pe.sections:
            if not section.IMAGE_SCN_MEM_EXECUTE:  # 如果不可执行那么就检查下一个段
                continue
            section_data = section.get_data()
            section_addr = section.VirtualAddress
            for ins in md.disasm(section_data, section_addr):
                opcodeList.append(ins.mnemonic)
    except CustomError('Can not disassemble ' + validPath + ', return None. Check the configs of PEFILE and Capstone'):
        return None

    bufferSet = set()
    for index in range(len(opcodeList) - n + 1):
        keyValue = []
        for window in range(n):
            keyValue.append(opcodeList[index + window])
        keyValue = ' '.join(keyValue)
        bufferSet.add(keyValue)  # a n-gram element，'mov ret jmp', 具备去重属性

    e = time.time()
    print('n grams length:', n, ', files name:', validPath, ", opcode length:", len(opcodeList),
          ', time cost:', round(e - s, 6), ', parsed by process:', os.getpid())

    return list(bufferSet)
    # 此时完成1整个二进制文件的n-grams读取，即统计了此样本都存在哪些n-grams，但没有计数出现频数


def n_grams_frequency(n: int, rootPaths: str):
    """
    计算rootPaths中(isMalicious or not), 特定n-gram在所有files中出现的次数
    """
    paths = os.listdir(rootPaths)
    validPaths = {}
    for path in paths:
        fullPath = os.path.join(rootPaths, path)
        validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)
    # 对于进程池策略来说，需要先读大文件，所以对其进行降序排序
    fileParsePool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    # 对于文件读取的多进程待实现，线程池实现
    resultAddress = []
    for validPath in validPaths.keys():  # 对于某个文件来说
        resultAddress.append(fileParsePool.apply_async(fileParse, args=(n, validPath)))
    fileParsePool.close()
    fileParsePool.join()
    gramsSequence = []
    failure = 0
    for address in resultAddress:
        result = address.get()
        if result is None:
            failure += 1
            continue
        gramsSequence += result
    # 返回的第二个元素是此目录下，可正常进行反汇编的数量，这是可真正计入总文档数量的
    return [gramsSequence, len(validPaths.keys()) - failure]


def main():
    # step 1: 统计不同n-grams(论文中n ∈ [1, 2, 3])在恶意样本和良性样本中的出现频率初始化恶意样本、良性样本以及总样本的数量
    print('Start to extract n-grams of opcodes using processpool:')
    totalTimeStart = time.time()

    with open("../config.json") as f:
        configs = json.load(f)
    maliciousPath = configs['malicious_validation_samples_path']  # malicious validation set path
    benignPath = configs['benign_validation_samples_path']  # benign validation set path

    # 全局字典变量存储数据，如下：
    malicious = {}  # 统计在所有恶意样本中，不同n取值下每种grams出现的频率
    benign = {}  # 统计在所有良性样本中，不同n取值下每种grams出现的频率
    mal_grams = []
    ben_grams = []
    for n in [1, 2, 3]:
        mal_result = n_grams_frequency(n, maliciousPath)
        mal_grams += mal_result[0]
        maliciousSamples = mal_result[1]  # 真实可用的恶意样本数量，会被重复赋值三次，每次赋相同的数值

        ben_result = n_grams_frequency(n, benignPath)
        ben_grams += ben_result[0]
        benignSamples = ben_result[1]  # 真实可用的良性样本数量，会被重复赋值三次，每次赋相同的数值

    allSamples = maliciousSamples + benignSamples  # 样本总数量
    HS = entropy(maliciousSamples / allSamples) + entropy(benignSamples / allSamples)
    print('disassemble malSamples:', maliciousSamples, ', disassemble beniSamples:',
          benignSamples, ', disassemble totalSamples:', allSamples, ', originalEntropy: ', HS)

    totalTimeEnd = time.time()
    print('Extracting n-grams of opcodes ends, Total time cost:', round(totalTimeEnd - totalTimeStart, 6))
    print()

    # step 3:在所有的mal or benign的txt buffer文件中进行数据合并，并更新对应字典
    print('Calculate the frequency for each n-gram and kept in dicts:')
    dictUpdate(malicious, mal_grams)
    dictUpdate(benign, ben_grams)
    print('Calculation of n-gram frequency ends')
    print()

    # step 4:针对字典的初步筛选，将出现频率少的筛出
    print("First selection of feature dicts by threshold:")
    allKeys = set(list(malicious.keys()) + list(benign.keys()))
    # assert len(malicious.keys()) == len(allKeys)  # 如果二者不相同，那么存在完美特征
    threshold = int(allSamples * 0.01)  # 筛选阈值

    maliciousOnly = r'record/maliciousOnlyRecord.txt'  # 记录特殊情况
    benignOnly = r'record/benignOnlyRecord.txt'  # 记录特殊情况
    for item in [maliciousOnly, benignOnly]:
        if os.path.exists(item):
            os.remove(item)

    writeMalStream = open(maliciousOnly, 'a')
    writeBenignStream = open(benignOnly, 'a')
    for key in allKeys:
        if key in malicious.keys() and key in benign.keys():
            appearanceNums = malicious[key] + benign[key]
            if appearanceNums <= threshold:
                malicious.pop(key)
                benign.pop(key)

        # 样本数量越多，那对于某个feature来说, 其仅出现在malicious or benign中的概率越小
        # 如果出现，则说明，此特征在此实验下是较优特征，但是一般取不到
        elif key in malicious.keys() and key not in benign.keys():  # 若样本足够大，无法实现此情况
            if malicious[key] <= threshold:  # 仅出现在恶意样本中，但出现频率小于总样本数量的10%
                malicious.pop(key)
            else:  # 仅出现在恶意样本中，且出现频率大于总样本数量的10%。此时需要特殊处理
                message = '|' + key + '|' + ' only appears in malicious samples, rate in allSamples = ' + str(
                    malicious[key] / allSamples) + '\n'
                writeMalStream.writelines(message)
                # raise CustomException('Find perfect feature!')

                malicious.pop(key)  # 为了测试，也先pop处理，实际运行时使用上一条指令

        elif key not in malicious.keys() and key in benign.keys():  # 若样本足够大，无法实现此情况
            if benign[key] <= threshold:  # 仅出现在良性样本中，但出现频率小于总样本数量的10%
                benign.pop(key)
            else:  # 仅出现在良性样本中，且出现频率大于总样本数量的10%。此时需要特殊处理
                message = '|' + key + '|' + ' only appears in benign samples, rate in allSamples = ' + str(
                    benign[key] / allSamples) + '\n'
                writeBenignStream.writelines(message)
                # raise CustomException('Find perfect feature!')

                benign.pop(key)  # 为了测试，也先pop处理，实际运行时使用上一条指令

    writeMalStream.close()
    writeBenignStream.close()
    print('Some special features are recorded in txt files under ' + os.getcwd() + '\\record Directory')
    print("First selection of feature dicts by threshold ends:")
    print()

    assert len(malicious) == len(benign)  # 经过初步筛选后，malicious应该与benign相同
    # step 5:下面开始计算所有n-grams特征的信息增益
    print("Calculate the InformationGain for each key in dicts and only "
          "keep the top " + configs["max_features_length"] + ' keys with highest IG')
    InfoGain = {}  # 计算每个 unique n-gram 的信息增益
    allKeys = set(list(malicious.keys()) + list(benign.keys()))  # 重新计算筛选过的字典keys
    for key in allKeys:
        # key is the feature name, boolean type
        # 经过第一部筛选，此时可以保证key在mal and benign中都至少出现一次
        key1mal1 = malicious[key]  # key在恶意样本(mal = 1)中出现（key = 1）的次数，由于设计缘故（出发点是统计出现的元素），其最小值为1
        key0mal1 = maliciousSamples - key1mal1  # 虽然是恶意样本（mal=1）, 但key没有出现（key = 0）的次数，取值可能为0

        key1mal0 = benign[key]  # key在良性样本(mal = 0)中出现（key = 1）的次数，由于设计缘故（出发点是统计出现的元素），其最小值为1
        key0mal0 = benignSamples - key1mal0  # 虽然是良性样本（mal = 0）, 但key没有出现（key = 0）的次数，取值可能为0
        # 注意，只有key0mal1和key0mal0有取值为0的可能

        # ****这一部分确保计算熵时，参数不为0，实际上二条语句只保留第二条即可
        if key0mal1 == 0 and key0mal0 == 0:  # 只有这两个有可能取到0
            continue  # 特征无用，即key只能取 == 1一种值
        elif key0mal0 == 0 or key0mal1 == 0:
            # print('find good feature' + key)
            # 较好特征，例如[3,0,1,1]，这表示当key取1时，可以是mal（3个）或benign（1个），
            # 而key取0时，只能是benign，但一般取不到，且计算entropy也有问题
            continue
        # ****这一部分确保计算熵时，参数不为0

        assert key1mal1 + key0mal1 + key1mal0 + key0mal0 == allSamples
        # print(key1mal1, key0mal1, key1mal0, key0mal0)

        key1 = key1mal1 + key1mal0  # 即此bool特征 = 1的样本个数
        key0 = allSamples - key1  # 即此bool特征 = 为0的样本个数

        pi1 = key1mal1 / key1  # 即此bool特征=1情况下(此特征出现的情况下)，恶意样本占比
        pi2 = key1mal0 / key1  # 即此bool特征=1情况下(此特征出现的情况下)，良性样本占比

        pi3 = key0mal1 / key0  # 即此bool特征=0情况下(此特征出现的情况下)，恶意样本占比
        pi4 = key0mal0 / key0  # 即此bool特征=0情况下(此特征出现的情况下)，良性样本占比

        updatedEntropy = key1 / allSamples * (entropy(pi1) + entropy(pi2)) + key0 / allSamples * (
                entropy(pi3) + entropy(pi4))

        InfoGain[key] = round(HS - updatedEntropy, 6)  # IG值只保留6位

    print('Original IG dict size:', len(InfoGain))
    InfoGain = dict(list(dictSort(InfoGain).items())[:int(configs["max_features_length"])])  # 降序排列且只取前13,000个元素
    print('Selected IG dict size:', len(InfoGain))

    jsonPath = 'feature-IG-pairs.json'
    if os.path.exists(jsonPath):
        os.remove(jsonPath)
    with open(jsonPath, 'w') as json_file:
        json.dump(InfoGain, json_file)  # 应该注意的是，运行多次生成的文件可以是不同，因为IG重复值很多，然后降序排序后的结果不唯一

    print("Successfully output the feature-IG-pairs.json as the final features for opcode grams")
    print()
