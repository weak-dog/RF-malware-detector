import os
import pathlib
import time
import multiprocessing
import csv
import pefile


class CustomError(Exception):
    """自定义异常类型的示例"""

    def __init__(self, message):
        self.message = message
        super().__init__(self.message)


def dictSort(dictionary):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def extract_feature(validPath: str):
    s = time.time()
    try:
        pe = pefile.PE(validPath)
        optional_header = pe.OPTIONAL_HEADER
        COFF_header = pe.FILE_HEADER  # COFF隶属于此

        ImageBase = optional_header.ImageBase
        AddressOfEntryPoint = optional_header.AddressOfEntryPoint
        SizeOfImage = optional_header.SizeOfImage
        SizeOfCode = optional_header.SizeOfCode
        BaseOfCode = optional_header.BaseOfCode
        SizeOfInitializedData = optional_header.SizeOfInitializedData
        SizeOfUninitializedData = optional_header.SizeOfUninitializedData

        # The optional header magic number determines whether an image is a PE32 or PE32+ executable.
        if optional_header.Magic == pefile.OPTIONAL_HEADER_MAGIC_PE:  # PE32才有此字段
            BaseOfData = optional_header.BaseOfData
        else:  # PE32+没有此字段，所以值为置为-1
            BaseOfData = -1

        SizeOfHeaders = optional_header.SizeOfHeaders
        SectionAlignment = optional_header.SectionAlignment
        FileAlignment = optional_header.FileAlignment
        NumberOfSections = COFF_header.NumberOfSections  # xal.header.numberof_sections
        SizeOfOptionalHeader = COFF_header.SizeOfOptionalHeader  # xal.header.sizeof_optional_header

        # 需要特殊处理的特征
        # xal.header.characteristics.value
        Characteristics = bin(COFF_header.Characteristics)[2:].zfill(16)  # 字符串类型
        buffer = []
        for i in range(len(Characteristics)):
            buffer.append(int(Characteristics[i]))
        Characteristics = buffer  # 更改为list类型

        featureList = [ImageBase, AddressOfEntryPoint, SizeOfImage, SizeOfCode, BaseOfCode,
                       SizeOfInitializedData, SizeOfUninitializedData, BaseOfData, SizeOfHeaders,
                       SectionAlignment, FileAlignment, NumberOfSections, SizeOfOptionalHeader] + Characteristics

    except CustomError('Can not extract headers of ' + validPath + ', fill the features with \'-1\''):
        featureList = [-1] * 29

    featureList += [pathlib.Path(validPath).parts[-1]]  # 首先给列加上名字，可选

    # 在train和validation中，存在malicious和benign目录，可以知晓标签
    # 但是在test中没有这两个目录，即不知道标签，所以无法加上标签
    if pathlib.Path(validPath).parts[-2] == 'malicious':
        featureList += [1]  # 倒数第一列是label
    elif pathlib.Path(validPath).parts[-2] == 'benign':
        featureList += [0]  # 倒数第一列是label

    e = time.time()
    print('Extract:', validPath, ' ends, time cost:', round(e - s, 6))
    return featureList


# 此函数主要是为了实现处理malicious和benign时的代码复用
def classParse(features: list, rootPath: str, csvPath: str):
    if os.path.exists(csvPath):
        os.remove(csvPath)

    csvStream = open(csvPath, mode='a', newline='')
    csvWriter = csv.writer(csvStream)
    if pathlib.Path(rootPath).parts[-1] != 'test':  # 说明此时是为训练集或验证集提取特征
        csvWriter.writerow(features + ['name'] + ['label'])  # 此处是bug更改位置
        csvStream.close()  # 由于特征提取过程过慢，所以先关闭文件流
        print('feature columns: ', [features[0], features[1], '...', features[-2], features[-1], 'name', 'label'])
    else:  # 说明此时是为测试集提取特征
        csvWriter.writerow(features + ['name'])
        csvStream.close()  # 由于特征提取过程过慢，所以先关闭文件流
        print('feature columns: ', [features[0], features[1], '...', features[-2], features[-1], 'name'])

    validPaths = {}
    for path in os.listdir(rootPath):
        fullPath = os.path.join(rootPath, path)
        validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)  # 降序排列

    extractPool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    csvRows = []
    for validPath in validPaths.keys():  # 没有使用validPaths.keys(), 但但效果相同
        csvRows.append(extractPool.apply_async(extract_feature, args=(validPath,)))
    extractPool.close()
    extractPool.join()

    csvStream = open(csvPath, mode='a', newline='')
    csvWriter = csv.writer(csvStream)
    for row in csvRows:
        csvWriter.writerow(row.get())
    csvStream.close()
