import os
import pathlib
import time
import multiprocessing
import csv
from collections import Counter
from scipy.stats import entropy
import lief
import pefile
import numpy as np

# 全局变量
standard_names = ['.bss', '.cormeta', '.data', '.debug', '.debug$F', '.debug$P', '.debug$S', '.debug$T',
                  '.drective', '.edata', '.idata', '.idlsym', '.pdata', '.rdata', '.reloc',
                  '.rsrc', '.sbss', '.srdata', '.text', '.tls', '.tls$', '.vsdata', '.xdata']


# standard_names = ['.text', '.data', '.rdata', '.bss', '.idata', '.edata', '.pdata', '.rsrc', '.reloc', '.debug']
class CustomError(Exception):
    """自定义异常类型的示例"""

    def __init__(self, message):
        self.message = message
        super().__init__(self.message)


def dictSort(dictionary):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def GetEntropy(byte_array: bytes, total_bytes: int):
    byte_counts = Counter(byte_array)
    # 计算概率分布
    probabilities = [count / total_bytes for count in byte_counts.values()]
    # 计算熵
    entropy_value = entropy(probabilities, base=2)  # 使用2作为底数计算熵
    return entropy_value


def GetMaxMeanMin(data: list):
    return max(data), np.mean(data), min(data)


def extract_feature(validPath: str, features, max_section_num: int):
    s = time.time()
    featureList = []
    try:
        # first block features: Processed_resources features
        pe = lief.PE.parse(validPath)
        if pe.has_resources:
            root = pe.resources
            resource_num = 0
            resource_entropy = []
            resource_size = []
            for level1 in root.childs:
                for level2 in level1.childs:
                    for level3 in level2.childs:  # 假设每个资源是三级目录结构
                        assert level3.is_data
                        resource_num += 1
                        byte_array = level3.content.tobytes()
                        byte_size = level3.content.nbytes
                        resource_entropy.append(GetEntropy(byte_array, byte_size))
                        resource_size.append(byte_size)
            resourcesMaxEntropy, resourcesMeanEntropy, resourcesMinEntropy = GetMaxMeanMin(resource_entropy)
            resourcesMaxSize, resourcesMeanSize, resourcesMinSize = GetMaxMeanMin(resource_size)
            featureList += [resource_num,
                            resourcesMaxEntropy, resourcesMaxSize,
                            resourcesMeanEntropy, resourcesMeanSize,
                            resourcesMinEntropy, resourcesMinSize]
        else:
            print(validPath + ' has no .rsrc, fill these features with \'-1\'')
            featureList += [-1] * 7
        del pe

    except CustomError('Can not extract resources of ' + validPath + ', fill these features with \'-1\''):
        featureList += [-1] * 7
    try:
        pe = pefile.PE(validPath)
        # second block features: Processed_sections features
        sectionsEntropy = [section.get_entropy() for section in pe.sections]
        sectionsSize = [section.SizeOfRawData for section in pe.sections]
        sectionsVirtualSize = [section.Misc_VirtualSize for section in pe.sections]

        sectionsMaxEntropy, sectionsMeanEntropy, sectionsMinEntropy = GetMaxMeanMin(sectionsEntropy)
        sectionsMaxSize, sectionsMeanSize, sectionsMinSize = GetMaxMeanMin(sectionsSize)
        sectionsMaxVirtualSize, sectionsMeanVirtualSize, sectionsMinVirtualSize = GetMaxMeanMin(sectionsVirtualSize)

        featureList += [sectionsMaxEntropy, sectionsMaxSize, sectionsMaxVirtualSize,
                        sectionsMeanEntropy, sectionsMeanSize, sectionsMeanVirtualSize,
                        sectionsMinEntropy, sectionsMinSize, sectionsMinVirtualSize]

        # third block features
        sections_i = []
        # 可以进行有效计算的段只有以下这些
        for i in range(min(len(pe.sections), max_section_num)):
            buffer = [1]  # Section_i_exists == True
            buffer.append(1) if pe.sections[i].Name.decode().strip('\x00') in standard_names else buffer.append(0)
            buffer.append(pe.sections[i].SizeOfRawData)
            buffer.append(pe.sections[i].Misc_PhysicalAddress)  # ImageBase + virtual_address
            buffer.append(pe.sections[i].Misc_VirtualSize)
            buffer.append(pe.sections[i].get_entropy())
            buffer.append(pe.sections[i].NumberOfRelocations)
            buffer.append(pe.sections[i].PointerToRelocations)

            Characteristics = bin(pe.sections[i].Characteristics)[2:].zfill(32)  # 字符串类型
            for bit in range(len(Characteristics)):
                buffer.append(int(Characteristics[bit]))

            sections_i += buffer

        overPartTemplate = [0, -1, -1, -1, -1, -1, -1, -1] + [-1] * 32  # 对于不存在的段，i_exists有意义外，其他置为-1
        if max_section_num > len(pe.sections):
            sections_i += overPartTemplate * (max_section_num - len(pe.sections))

        featureList += sections_i
    except CustomError('Can not extract sections of ' + validPath + ', fill the features with \'-1\''):
        featureList += [-1] * (len(features) - 7)

    featureList += [pathlib.Path(validPath).parts[-1]]  # 首先给列加上名字，可选

    # 在train和validation中，存在malicious和benign目录，可以知晓标签
    # 但是在test中没有这两个目录，即不知道标签，所以无法加上标签
    if pathlib.Path(validPath).parts[-2] == 'malicious':
        featureList += [1]  # 倒数第一列是label
    elif pathlib.Path(validPath).parts[-2] == 'benign':
        featureList += [0]  # 倒数第一列是label

    e = time.time()
    print('Extract:', validPath, ' ends, time cost:', e - s)
    return featureList


# 此函数主要是为了实现处理malicious和benign时的代码复用
def classParse(features: list, max_section_num: int, rootPath: str, csvPath: str):
    if os.path.exists(csvPath):
        os.remove(csvPath)

    csvStream = open(csvPath, mode='a', newline='')
    csvWriter = csv.writer(csvStream)
    if pathlib.Path(rootPath).parts[-1] != 'test':  # 说明此时是为训练集或验证集提取特征
        csvWriter.writerow(features + ['name'] + ['label'])  # 此处是bug更改位置
        csvStream.close()  # 由于特征提取过程过慢，所以先关闭文件流
        print('feature columns: ', [features[0], features[1], '...', features[-2], features[-1], 'name', 'label'])
    else:  # 说明此时是为测试集提取特征
        csvWriter.writerow(features + ['name'])
        csvStream.close()  # 由于特征提取过程过慢，所以先关闭文件流
        print('feature columns: ', [features[0], features[1], '...', features[-2], features[-1], 'name'])

    validPaths = {}
    for path in os.listdir(rootPath):
        fullPath = os.path.join(rootPath, path)
        validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)  # 降序排列

    extractPool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    csvRows = []
    for validPath in validPaths.keys():
        csvRows.append(extractPool.apply_async(extract_feature, args=(validPath, features, max_section_num)))
    extractPool.close()
    extractPool.join()

    csvStream = open(csvPath, mode='a', newline='')
    csvWriter = csv.writer(csvStream)
    for row in csvRows:
        csvWriter.writerow(row.get())
    csvStream.close()
