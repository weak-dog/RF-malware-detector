import collections
import math
import os
import time
import multiprocessing
import json


def entropy(pi):
    return -pi * math.log2(pi)


def dictSort(dictionary):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def dictUpdate(dictionary: dict, grams: list):
    s = time.time()
    element_counts = collections.Counter(grams)
    for key, value in element_counts.items():
        dictionary[key] = value
    e = time.time()
    print('Dict size:', len(dictionary), ', calculation time:', round(e - s, 6))


def fileParse(n: int, validPath: str):
    s = time.time()
    # 读取任意文件的二进制内容，新适用版本，不存在无法处理的文件类型
    # 所以byte_grams不需要像opcode_grams那样考虑多种的异常情况
    with open(validPath, "rb") as f:
        byteList = f.read()  # 为了更好地进行窗口取值，所以保持使用bytes的格式，而不是使用.hex()转换成字符串

    bufferSet = set()
    for index in range(len(byteList) - n + 1):
        keyValue = ''
        for window in range(n):
            keyValue += hex(byteList[index + window])[2:].zfill(2)  # 转换成16进制后手动补零
        bufferSet.add(keyValue)  # a n-gram element，具备去重属性

    e = time.time()
    print('n grams length:', n, ', files name:', validPath, ", byte length:", len(byteList),
          ', time cost:', round(e - s, 6), ', parsed by process:', os.getpid())

    return list(bufferSet)
    # 此时完成1整个二进制文件的n-grams读取，即统计了此样本都存在哪些n-grams，但没有计数出现频数


def n_grams_frequency(n: int, rootPaths: str):
    """
    计算rootPaths中(isMalicious or not), 特定n-gram在所有files中出现的次数
    """
    paths = os.listdir(rootPaths)
    validPaths = {}
    for path in paths:
        fullPath = os.path.join(rootPaths, path)
        validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)
    # 对于进程池策略来说，需要先读大文件，所以对其进行降序排序
    fileParsePool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    # 对于文件读取的多进程待实现，线程池实现
    resultAddress = []
    for validPath in validPaths.keys():  # 对于某个文件来说
        resultAddress.append(fileParsePool.apply_async(fileParse, args=(n, validPath)))
    fileParsePool.close()
    fileParsePool.join()
    gramsSequence = []
    for address in resultAddress:
        gramsSequence += address.get()
    return gramsSequence


def main():
    # step 1: 初始化恶意样本、良性样本以及总样本的数量
    print('Output initial information before byteGram_feature_selection:')
    with open("../config.json") as f:
        configs = json.load(f)
    maliciousPath = configs['malicious_validation_samples_path']  # malicious validation set path
    benignPath = configs['benign_validation_samples_path']  # benign validation set path

    maliciousSamples = len(os.listdir(maliciousPath))  # 恶意样本的数量
    benignSamples = len(os.listdir(benignPath))  # 良性样本的数量
    allSamples = maliciousSamples + benignSamples  # 样本总数量
    HS = entropy(maliciousSamples / allSamples) + entropy(benignSamples / allSamples)
    print('malSamples num:', maliciousSamples, ', beniSamples num:', benignSamples,
          ', totalSamples nmu:', allSamples, ', originalEntropy: ', HS)
    print('Output initial information ends:')
    print()

    # step 2: 统计不同n-grams(论文中n ∈ [4, 5, 6])在恶意样本和良性样本中的出现频率
    # 全局字典变量存储数据，如下：
    print('Start to extract n-grams of bytes using processpool:')
    totalTimeStart = time.time()
    malicious = {}  # 统计在所有恶意样本中，不同n取值下每种grams出现的频率
    benign = {}  # 统计在所有良性样本中，不同n取值下每种grams出现的频率
    mal_grams = []
    ben_grams = []
    for n in [4, 5, 6]:
        mal_grams += n_grams_frequency(n, maliciousPath)
        ben_grams += n_grams_frequency(n, benignPath)
    totalTimeEnd = time.time()
    print('Extracting n-grams of bytes ends, Total time cost:', round(totalTimeEnd - totalTimeStart, 6))
    print()

    # step 3:在所有的mal or benign的txt buffer文件中进行数据合并，并更新对应字典
    print('Calculate the frequency for each n-gram and kept in dicts:')
    dictUpdate(malicious, mal_grams)  # 更新字典时，用过的文件会被删除
    dictUpdate(benign, ben_grams)  # 更新字典时，用过的文件会被删除
    print('Calculation of n-gram frequency ends')
    print()

    # step 4:针对字典的初步筛选，将出现频率少的筛出
    print("First selection of feature dicts by threshold:")
    allKeys = set(list(malicious.keys()) + list(benign.keys()))
    # assert len(malicious.keys()) == len(allKeys)  # 如果二者不相同，那么存在完美特征
    threshold = int(allSamples * 0.01)  # 筛选阈值

    maliciousOnly = r'record/maliciousOnlyRecord.txt'  # 记录特殊情况
    benignOnly = r'record/benignOnlyRecord.txt'  # 记录特殊情况
    for item in [maliciousOnly, benignOnly]:
        if os.path.exists(item):
            os.remove(item)

    writeMalStream = open(maliciousOnly, 'a')
    writeBenignStream = open(benignOnly, 'a')
    for key in allKeys:
        if key in malicious.keys() and key in benign.keys():
            appearanceNums = malicious[key] + benign[key]
            if appearanceNums <= threshold:
                malicious.pop(key)
                benign.pop(key)

        # 样本数量越多，那对于某个feature来说, 其仅出现在malicious or benign中的概率越小
        # 如果出现，则说明，此特征在此实验下是较优特征，但是一般取不到
        elif key in malicious.keys() and key not in benign.keys():  # 若样本足够大，无法实现此情况
            if malicious[key] <= threshold:  # 仅出现在恶意样本中，但出现频率小于总样本数量的10%
                malicious.pop(key)
            else:  # 仅出现在恶意样本中，且出现频率大于总样本数量的10%。此时需要特殊处理
                message = '|' + key + '|' + ' only appears in malicious samples, rate in allSamples = ' + str(
                    malicious[key] / allSamples) + '\n'
                writeMalStream.writelines(message)
                # raise CustomException('Find perfect feature!')

                malicious.pop(key)  # 为了测试，也先pop处理，实际运行时使用上一条指令

        elif key not in malicious.keys() and key in benign.keys():  # 若样本足够大，无法实现此情况
            if benign[key] <= threshold:  # 仅出现在良性样本中，但出现频率小于总样本数量的10%
                benign.pop(key)
            else:  # 仅出现在良性样本中，且出现频率大于总样本数量的10%。此时需要特殊处理
                message = '|' + key + '|' + ' only appears in benign samples, rate in allSamples = ' + str(
                    benign[key] / allSamples) + '\n'
                writeBenignStream.writelines(message)
                # raise CustomException('Find perfect feature!')

                benign.pop(key)  # 为了测试，也先pop处理，实际运行时使用上一条指令

    writeMalStream.close()
    writeBenignStream.close()
    print('Some special features are recorded in txt files under ' + os.getcwd() + '\\record Directory')
    print("First selection of feature dicts by threshold ends:")
    print()

    assert len(malicious) == len(benign)  # 经过初步筛选后，malicious应该与benign相同
    # step 5:下面开始计算所有n-grams特征的信息增益
    print("Calculate the InformationGain for each key in dicts and only "
          "keep the top " + configs["max_features_length"] + ' keys with highest IG')
    InfoGain = {}  # 计算每个 unique n-gram 的信息增益
    allKeys = set(list(malicious.keys()) + list(benign.keys()))  # 重新计算筛选过的字典keys
    for key in allKeys:
        # key is the feature name, boolean type
        # 经过第一部筛选，此时可以保证key在mal and benign中都至少出现一次
        key1mal1 = malicious[key]  # key在恶意样本(mal = 1)中出现（key = 1）的次数，由于设计缘故（出发点是统计出现的元素），其最小值为1
        key0mal1 = maliciousSamples - key1mal1  # 虽然是恶意样本（mal=1）, 但key没有出现（key = 0）的次数，取值可能为0

        key1mal0 = benign[key]  # key在良性样本(mal = 0)中出现（key = 1）的次数，由于设计缘故（出发点是统计出现的元素），其最小值为1
        key0mal0 = benignSamples - key1mal0  # 虽然是良性样本（mal = 0）, 但key没有出现（key = 0）的次数，取值可能为0
        # 注意，只有key0mal1和key0mal0有取值为0的可能

        # ****这一部分确保计算熵时，参数不为0，实际上二条语句只保留第二条即可
        if key0mal1 == 0 and key0mal0 == 0:  # 只有这两个有可能取到0
            continue  # 特征无用，即key只能取 == 1一种值
        elif key0mal0 == 0 or key0mal1 == 0:
            # print('find good feature' + key)
            # 较好特征，例如[3,0,1,1]，这表示当key取1时，可以是mal（3个）或benign（1个），
            # 而key取0时，只能是benign，但一般取不到，且计算entropy也有问题
            continue
        # ****这一部分确保计算熵时，参数不为0

        assert key1mal1 + key0mal1 + key1mal0 + key0mal0 == allSamples
        # print(key1mal1, key0mal1, key1mal0, key0mal0)

        key1 = key1mal1 + key1mal0  # 即此bool特征 = 1的样本个数
        key0 = allSamples - key1  # 即此bool特征 = 为0的样本个数

        pi1 = key1mal1 / key1  # 即此bool特征=1情况下(此特征出现的情况下)，恶意样本占比
        pi2 = key1mal0 / key1  # 即此bool特征=1情况下(此特征出现的情况下)，良性样本占比

        pi3 = key0mal1 / key0  # 即此bool特征=0情况下(此特征出现的情况下)，恶意样本占比
        pi4 = key0mal0 / key0  # 即此bool特征=0情况下(此特征出现的情况下)，良性样本占比

        updatedEntropy = key1 / allSamples * (entropy(pi1) + entropy(pi2)) + key0 / allSamples * (
                    entropy(pi3) + entropy(pi4))

        InfoGain[key] = round(HS - updatedEntropy, 6)  # IG值只保留6位

    print('Original IG dict size:', len(InfoGain))

    # 于此处插入elbow method来获取IG的threshold

    InfoGain = dict(list(dictSort(InfoGain).items())[:int(configs["max_features_length"])])  # 降序排列且只取前13,000个元素
    print('Selected IG dict size:', len(InfoGain))

    jsonPath = 'feature-IG-pairs.json'
    if os.path.exists(jsonPath):
        os.remove(jsonPath)
    with open(jsonPath, 'w') as json_file:
        json.dump(InfoGain, json_file)  # 应该注意的是，运行多次生成的文件可以是不同，因为IG重复值很多，然后降序排序后的结果不唯一

    print("Successfully output the feature-IG-pairs.json as the final features for byte grams")
    print()


if __name__ == '__main__':
    main()
