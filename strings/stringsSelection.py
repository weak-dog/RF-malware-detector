import collections
import json
import multiprocessing
import os
import time


def dictSort(dictionary: dict):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def dictFilter(dictionary: dict, threshold: int):
    """
    根据阈值，将dict中value>=threshold的元素保留下来，这是筛选特征的最后一步
    """
    return {key: value for key, value in dictionary.items() if value >= threshold}


def dictUpdate(dictionary: dict, allStrings: list):  # txtRoots会统计所有txt结尾的文件
    s = time.time()
    stringCounts = collections.Counter(allStrings)
    for key, value in stringCounts.items():
        dictionary[key] = value
    e = time.time()
    print('dict size:', len(dictionary), ', update time:', round(e - s, 6))


def fileParse(validPath: str):
    s = time.time()

    cmdName = 'strings ' + validPath
    fileStream = os.popen(cmdName)  # strings工具默认筛选长度为4
    result = fileStream.read()  # read()后的结果是一个长str, 以'\n'作为分割标志
    fileStream.close()
    result = list(set(result.split('\n')[:-1]))  # 于此处去重
    e = time.time()

    print('files name:', validPath, ', unique string nums:', len(result),
          ', time cost:', round(e - s, 6), ', parsed by process:', os.getpid())

    return result


# 此函数主要是为了实现处理malicious和benign时的代码复用
def classParse(rootPath: str):
    paths = os.listdir(rootPath)

    validPaths = {}
    for path in paths:
        fullPath = os.path.join(rootPath, path)
        validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)

    fileParsePool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    resultAddress = []
    for validPath in validPaths.keys():
        resultAddress.append(fileParsePool.apply_async(fileParse, args=(validPath,)))
    fileParsePool.close()
    fileParsePool.join()

    stringsSequence = []
    for address in resultAddress:
        stringsSequence += address.get()
    return stringsSequence


def main():
    # 多进程版本, step 1:
    # 事实上，对于strings来说，由于不需要计算IG这样与正负样本有关的筛选标准，
    # 所以不需要把正负样本分开放，但为了逻辑统一，以及后续特征提取的便利，还是分开放
    with open("../config.json") as f:
        configs = json.load(f)

    print('Start to select string features by threshold:')

    maliciousPath = configs['malicious_validation_samples_path']  # malicious validation set path
    benignPath = configs['benign_validation_samples_path']  # benign validation set path

    maliciousSamples = len(os.listdir(maliciousPath))  # 恶意样本的数量
    benignSamples = len(os.listdir(benignPath))  # 良性样本的数量
    allSamples = maliciousSamples + benignSamples
    print('malicious nums:', maliciousSamples, ', benign nums:', benignSamples, ', total nums:', allSamples)

    allStrings = []
    allStrings += classParse(maliciousPath)
    allStrings += classParse(benignPath)

    # step 2
    stringDict = {}
    dictUpdate(stringDict, allStrings)  # 更新字典
    # print(stringDict)
    stringDict = dictSort(stringDict)  # 字典排序
    # print(stringDict)
    threshold = int(allSamples * 0.01)
    stringDict = dict(list(dictFilter(stringDict, threshold).items())[:int(configs["max_features_length"])])  # 过滤条件是>=
    print('Selected dict size:', len(stringDict), stringDict)

    # step 3
    jsonPath = 'feature-frequency-pairs.json'
    if os.path.exists(jsonPath):
        os.remove(jsonPath)
    with open(jsonPath, 'w') as json_file:
        json.dump(stringDict, json_file)  # 应该注意的是，运行多次生成的文件可以是不同，因为IG重复值很多，然后降序排序后的结果不唯一
    print('Selecting string features by threshold ends')
    print()