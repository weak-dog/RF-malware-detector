from collections import Counter
from scipy.stats import entropy
import os
import pathlib
import time
import multiprocessing
import csv


def dictSort(dictionary):
    return dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))


def extract_feature(validPath: str):
    s = time.time()
    with open(validPath, "rb") as f:
        byte_array = f.read()
    total_bytes = len(byte_array)

    # 使用 Counter 计算每个字节出现的频次
    byte_counts = Counter(byte_array)

    # 计算概率分布
    probabilities = [count / total_bytes for count in byte_counts.values()]

    # 计算熵
    entropy_value = entropy(probabilities, base=2)  # 使用2作为底数计算熵
    featureList = [total_bytes, entropy_value]

    featureList += [pathlib.Path(validPath).parts[-1]]  # 首先给列加上名字，可选

    # 在train和validation中，存在malicious和benign目录，可以知晓标签
    # 但是在test中没有这两个目录，即不知道标签，所以无法加上标签
    if pathlib.Path(validPath).parts[-2] == 'malicious':
        featureList += [1]  # 倒数第一列是label
    elif pathlib.Path(validPath).parts[-2] == 'benign':
        featureList += [0]  # 倒数第一列是label

    e = time.time()
    print('Extract:', validPath, ' ends, time cost:', round(e - s, 6))
    return featureList


def classParse(features: list, rootPath: str, csvPath: str):
    if os.path.exists(csvPath):
        os.remove(csvPath)

    csvStream = open(csvPath, mode='a', newline='')
    csvWriter = csv.writer(csvStream)
    if pathlib.Path(rootPath).parts[-1] != 'test':  # 说明此时是为训练集或验证集提取特征
        csvWriter.writerow(features + ['name'] + ['label'])  # 此处是bug更改位置
        csvStream.close()  # 由于特征提取过程过慢，所以先关闭文件流
        print('feature columns: ', [features[0], features[1], 'name', 'label'])
    else:  # 说明此时是为测试集提取特征
        csvWriter.writerow(features + ['name'])
        csvStream.close()  # 由于特征提取过程过慢，所以先关闭文件流
        print('feature columns: ', [features[0], features[1], 'name'])

    validPaths = {}
    for path in os.listdir(rootPath):
        fullPath = os.path.join(rootPath, path)
        validPaths[fullPath] = os.path.getsize(fullPath)
    validPaths = dictSort(validPaths)  # 降序排列

    extractPool = multiprocessing.Pool(multiprocessing.cpu_count())  # 分配进程数量
    csvRows = []
    for validPath in validPaths.keys():  # 没有使用validPaths.keys(), 但但效果相同
        csvRows.append(extractPool.apply_async(extract_feature, args=(validPath,)))
    extractPool.close()
    extractPool.join()

    csvStream = open(csvPath, mode='a', newline='')
    csvWriter = csv.writer(csvStream)
    for row in csvRows:
        csvWriter.writerow(row.get())
    csvStream.close()
